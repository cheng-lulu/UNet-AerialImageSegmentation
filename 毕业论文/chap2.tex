\begin{spacing}{2}
    \section{背景知识}
\end{spacing}
作为深度学习中一个常见的任务，基于语义的图像分割技术已有了长足的发展。现如今，一个通用的框架已确定，那就是在前端使用神经网络对特征进行提取，后端再利用传统图像处理方法如形态学处理、马尔可夫随机场等方法对前端的输出进行优化，输出分割图像。下面对使用卷积神经网络进行图像语义分割的技术做一个总结。
\subsection{全卷积网络}
图像分割方法可以总结为提取图像不同位置的特征，根据特征将图像分割。卷积神经网络的强大之处在于它可以自动学习特征。浅层的卷积层的感受野小，可以学习到一些局部的特征；深层的卷积层感受野大，可以学到更加抽象的特征。

我们从人工神经网络的应用历史可以知道，人工神经网络最开始用于图像分类。一个经典的用于图像分类的网络，如\cite{krizhevsky2012imagenet}中提出的AlexNet和\cite{simonyan2014very}中的VGG系列网络，通过在卷积层之后连接上若干个全连接层，将卷积层提取的特征图映射成一个固定长度的特征向量。这个特征向量在经过归一化处理之后，可以用来表示图像属于每一类的概率，进而达到对图像分类的目的。图像分割任务也可以被视作一个分类任务，即通过将像素分到不同的类别达到分割的效果。但是将卷积神经网络用于图像分割任务遇到了阻碍。直到2015年才有了重大突破。

利用卷积神经网络进行图像语义分割可以分为两个阶段，一个阶段是滑动窗口阶段，另外一个阶段是全卷积网络阶段。

一开始将卷积神经网络用于语义分割的方法是滑动窗口法。如\cite{ciresan2012deep}中就使用该方法对医疗图像进行分割。这个方法对某一个像素的分类步骤如下。首先我们会将待分类像素周围的一个图像块作为输入送入卷积神经网络。通过卷积神经网络对这一个区块的预测来决定该像素点属于哪一个分类。但这种方法有以下三个缺点。第一个缺点是这种方法所需要的存储空间会随着滑动窗口滑动次数的增加而飞速上升。其次，滑动窗口的大小存储空间也有很大的影响。第二个缺点是这种方法的计算效率十分低下。我们知道，在一副图像中，相互连接的像素点所取得图像块的内容基本上是重复的。所以这些基本重复的像素块在通过卷积神经网络之后得到的结果也基本上是重复的。这样子的话这个方法的计算效率不是很高。第三个缺点是感受野的大小在一开始就被限定为了索取图像块的大小。但是我们从前面可以知道，图像块的大小取得太大的话存储空间也需很大，所以我们只能取限定大小的图像块。但是一些高层语义信息在感受野受限的情况下是不能被正确提取的，所以这种方法的分类性能会受到限制。

2015年，加州大学伯克利分校的 Long 等人的论文\cite{long2015fully}发表。

全连接层在分类网络中的主要作用是将特征图映射到样本标记空间。$1\times 1$卷积核的卷积层可以看做一个逐像素的全连接层。它将深度维度的特征图映射成样本标记空间并保留了宽度和高度维度上的位置信息。由于在宽度和高度维度上使用$1\times 1$的卷积核，因此使用全卷积层的卷积神经网络还去掉了输入图像大小的限制，可以接受任意大小的输入图像。

用于分类任务的卷积神经网络中的全连接层将二维的图像压缩成了一维的特征向量从而丢失了定位像素的空间信息。而全卷积神经网络最后的$1\times 1$的卷积层在深度维度进行压缩，从而保留了位置信息。在卷积神经网络的最后接上卷积核大小为$1\times 1$的卷积层而非全连接层是用于图像语义分割的卷积神经网络的通用做法。例如在\cite{long2015fully}中，Long 等人对有广泛应用的分类网络 VGG16\cite{simonyan2014very} 进行改造，去掉了最后的全连接层，加上了全卷积层。在\cite{xie2015holistically}中Xie等人同样也对VGG系列网络做出修改，去掉了最后的全连接层转而使用卷积层。不论是用于场景解析的 PSPNet\cite{zhao2017pyramid} 和 SegNet\cite{badrinarayanan2017segnet} 还是用于医疗图像分割的 U-Net\cite{ronneberger2015u} 都使用全卷积层来替换全连接层来作为最后的输出层。

自论文\cite{long2015fully}发表之后，一大批受全卷积网络启发的神经网络开始涌现，人工神经网络开始广泛应用于图像语义分割任务。作为人工神经网络应用于图像语义分割的开山之作，全卷积网络现已有了广义和狭义两种定义。狭义上的全卷积网络指 Long 等人的论文\cite{long2015fully}中提出的用于图像语义分割的神经网络。而广义的全卷积网络则是指一类用全卷积层替代全连接层的神经网络。\cite{long2015fully}为我们提供了以下两个个使用卷积神经网络进行图像语义分割的启示：
\begin{enumerate}
    \item 去掉全连接层来保留位置信息；
    \item 利用浅层网络信息达到对图像精准分割的目的；
\end{enumerate}
下面我将对第二点做出阐释。

\subsection{使用全连接网络进行精准分割}
论文\cite{long2015fully}中提到，组合来自不同层的预测可以提高精度。使用神经网络提取的特征，在浅层是简单的图像特征，如边界、颜色等。而深层由于感受野增大，我们可以提取更抽象的特征。为了获得更精确的语义分割结果，我们需要结合浅层语义信息。

现在的图像分割网络结构可以分为对称结构（编码器-解码器结构）和线性结构两种。这两种网络利用浅层语义信息的方式是不同的。
\subsubsection{线性结构网络}
线性结构网络会在每组卷积结束后输出预测图。通过将不同层的卷积层上采样恢复到输入图像尺寸之后进行加操作得到最终的预测图像。在\cite{long2015fully}中，作者尝试了3种结合不同卷积层的方法。最后经过比较，发现将深层网络和浅层网络结合起来的输出图可以达到更精确的分割效果。

其次，线性结构网络还利用深监督获取浅层语义信息。深监督首次在\cite{lee2015deeply}中提出。深监督网络可以提高隐藏层学习过程的直接性和透明度。深监督网络的核心思想是为隐藏层提供集成的直接监督层，而不是仅在输出层提供监督。通过为每个隐藏层引入伴随目标函数来提供这种集成的直接隐藏层监督;这些伴随目标函数可以被视为学习过程中的附加（软）约束。引入深监督的语义分割网络有HEDNet\cite{xie2015holistically}。虽然这个网络是用于边缘检测，但是作者将边缘检测看做是一个图像分割问题：即将图像分割为边缘和非边缘两类。在这篇论文中，作者不光通过融合不同卷积层的输出生成最后的输出图像，还对不同卷积层同时计算误差，以所有卷积层的误差线性之和作为最后的损失函数。

\subsubsection{对称结构网络}
对称网络结构也是一种较为常见的网络结构。比较著名的网络有进行道路分割的SegNet\cite{badrinarayanan2017segnet}、PSPNet\cite{zhao2017pyramid}和下面将要使用的U-Net\cite{ronneberger2015u}。对称网络结构通常会将下采样的卷积层与上采样的卷积层在特征维度进行拼接，这样获得浅层网络信息。

池化层可以增加输入图像对一些小扰动的鲁棒性，更重要的是池化层增大了感受野，这对提取高层语义信息十分重要。但池化层与全连接层一样，对保留像素的位置信息是很不利的。因此如何恢复在池化层中丢失的位置信息是语义分割网络的重要议题。在\cite{long2015fully}中，Long 等人提出通过对输入数据进行上采样恢复到输入图像的大小来补充位置信息。

上采样的方式主要有三种。第一种是Long等人在\cite{long2015fully}使用的插值方法；第二种是转置卷积；第三种是DeepLab\cite{chen2014semantic}系列网络中使用的空洞卷积。

除插值操作之外，其他受\cite{long2015fully}启发的论文中也提出了其他对抗池化层的方法。如DeepLab\cite{chen2014semantic}系列网络中使用的空洞卷积。但使用空洞卷积计算成本较编码器-解码器方案高，因此不适用于图像巨大的遥感图像处理。U-Net网络中使用的则是反卷积方法，较空洞卷积更适合计算大型图像。

由于标注的原因，由于图像中一些小区域的标签不正确将会导致其无法匹配周围的像素标签。这时我们就需要对输出的分割图像做后优化。在\cite{badrinarayanan2017segnet}中，作者提出使用条件随机场对输出图像做优化。更一般的做法是训练几个超参数不同的神经网络，对这几个神经网络的预测图做或运算得到最终的输出图像。其次还有图像形态学运算可以除去预测图中的空洞及噪点。
